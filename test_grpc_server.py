import time
import grpc
from concurrent import futures
import sys
import os
from scraper.my_network_scraper import LinkedInMyNetworkScraper
from scraper.scraper_management import LinkedInScraperManager
from services.edit_profile import LinkedInProfileViewer

sys.path.append(os.path.join(os.path.dirname(__file__), 'proto'))
from proto import bot_pb2
from proto import bot_pb2_grpc

# Tr·∫°ng th√°i c√°c bot ƒëang ch·∫°y
active_bots = {}

class BotServiceServicer(bot_pb2_grpc.BotServiceServicer):
    def StreamBotCrawlUrl(self, request_iterator, context):
        print("request_iterator:", request_iterator)
        try:
            for command in request_iterator:
                print(f"üì• Received command: {command}")
                bot_id = command.bot_id
                cmd_type = command.type
                print(f"üì• Received command: {cmd_type} for bot_id={bot_id}")
                account = {
                    "LINKEDIN_EMAIL": "edwardwilson3512a47@gualues.com",
                    "LINKEDIN_PASSWORD": "Truong3979"
                }
                company_names = ["Vietnam", "VNG"]  
                if cmd_type == bot_pb2.BotCommand.START:
                    try:
                        scraper = LinkedInScraperManager(id=bot_id)
                        active_bots[bot_id] = scraper
                        print("üöÄ Kh·ªüi t·∫°o LinkedIn Scraper...")

                        if not scraper.initialize_driver():
                            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o driver")
                            return

                        if not scraper.login(account["LINKEDIN_EMAIL"], account["LINKEDIN_PASSWORD"]):
                            print("‚ùå ƒêƒÉng nh·∫≠p th·∫•t b·∫°i")
                            return

                        print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng! B·∫Øt ƒë·∫ßu scraping...")

                        # Stream logs from expand_and_collect_all_urls
                        # for log in scraper.my_connect_scraper.expand_and_collect_all_urls(bot_id=bot_id):
                        #     yield log

                        # print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng! B·∫Øt ƒë·∫ßu scraping...")
        
                        
                        
                        for index, company_name in enumerate(company_names):  # Iterate through the company names with index
                            print(f"\nüè¢ ƒêang x·ª≠ l√Ω c√¥ng ty: {company_name}")
                            edit_profile = LinkedInProfileViewer(scraper.driver, company_name=company_name)
                            
                            while True:  # Recursive loop
                                # Scrape my connect profiles
                                detailed_profiles = scraper.scrape_my_connect_profiles(bot_id=bot_id)
                                
                                if detailed_profiles:
                                    print(f"‚úÖ Ho√†n t·∫•t! Thu th·∫≠p ƒë∆∞·ª£c {len(list(detailed_profiles))} profiles chi ti·∫øt t·ª´ My Network")
                                else:
                                    print("‚ùå Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c th√¥ng tin n√†o t·ª´ My Network")
                                
                                # Click the close modal button and edit profile
                                print("üîÑ ƒêang ƒë√≥ng modal v√† ch·ªânh s·ª≠a profile...")
                                edit_profile.view_and_edit_profile()
                                
                                # Add a delay to avoid overwhelming the server
                                time.sleep(5)
                                
                                # Check if this is the last company in the array
                                if index == len(company_names) - 1:
                                    print(f"üè¢ C√¥ng ty cu·ªëi c√πng: {company_name}. Ti·∫øp t·ª•c thu th·∫≠p profiles...")
                                    continue  # Continue scraping profiles for the last company
                                
                                print(f"üè¢ ƒê√£ ho√†n th√†nh x·ª≠ l√Ω c√¥ng ty: {company_name}")
                                break 

                    except KeyboardInterrupt:
                        print("\n‚èπÔ∏è ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh theo y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.")
                    except Exception as e:
                        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
                        import traceback
                        traceback.print_exc()

                elif cmd_type == bot_pb2.BotCommand.STOP:
                    print(f"üõë Received stop command for bot_id={bot_id}")
                    if active_bots.get(bot_id):
                        scraper = active_bots[bot_id]
                        scraper.set_stop()
                        print(f"üõë Bot {bot_id} has been stopped.")
                        scraper.cleanup()
                    else:
                        yield bot_pb2.BotLog(
                            bot_id=bot_id,
                            message=f"[{bot_id}] ‚ö†Ô∏è No running bot found to stop."
                        )

        except Exception as e:
            print(f"‚ùå Error in StreamBotLogs: {e}")
            yield bot_pb2.BotLog(
                bot_id="unknown",
                message=f"‚ùå Error occurred: {str(e)}"
            )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    bot_pb2_grpc.add_BotServiceServicer_to_server(BotServiceServicer(), server)
    server.add_insecure_port('[::]:50051')
    print("üöÄ gRPC server started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    serve()
